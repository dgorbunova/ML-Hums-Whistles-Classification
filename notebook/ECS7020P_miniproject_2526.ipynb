{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k3WXNv5Qn5l"
      },
      "source": [
        "# [Your title goes here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaGn4ICrfqXZ"
      },
      "source": [
        "# 1 Author\n",
        "\n",
        "**Student Name:**  Daria Gorbunova\n",
        "\n",
        "**Student ID**: 231173199\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o38VQkcdKd6k"
      },
      "source": [
        "# 2 Problem formulation\n",
        "\n",
        "## 2.1 Type of problem:\n",
        "This machine learning problem is a multi-class classification one. The goal is to take a 10-second audio of a hum or a whistle, from the provided dataset, and determine which of the 8 possible song the audio is from. The possible song labels in this problem are:\n",
        "1. Happy\n",
        "2. Try Everything\n",
        "3. Remember Me\n",
        "4. New York\n",
        "5. Friend\n",
        "6. Necessities\n",
        "7. Feeling\n",
        "8. Married\n",
        "\n",
        "Since the input data (audio clip) has a label, this is a supervised learning task.\n",
        "\n",
        "## 2.2 The dataset:\n",
        "The dataset MLEnd Hums and Whistles II has been provided by the module team. The dataset was created with student contributions, where each student was tasked to provide:\n",
        "- 4 humming instances\n",
        "- 4 whistling instances\n",
        "\n",
        "per each of the 8 songs outlined above. \n",
        "\n",
        "However, some participants could not whistle and were instead given permission to supply 8 humming instances only. This led to uneven numbers of hums vs whistles in the raw collected data. \n",
        "After the examination of the subset (800 samples) provided for this mini project, using a file-counting script:\n",
        "- The dataset has 800 samples.\n",
        "- The number of instances of hums and whistles is balanced equally. \n",
        "- Each of the 8 song classes has 100 instances. \n",
        "\n",
        "Therefore, the subset of the dataset released by the teaching team for this task is balanced, which reduces the risk of class imbalance, which is ideal for machine learning models.\n",
        "\n",
        "## 2.3 Nature of instances:\n",
        "Each instance of the dataset consists of human-recorded raw waveform audio of differing lengths. Before providing the dataset, the module team applied basic preprocessing to clean the raw submissions. This included trimming of long silences, removing obvious background noise where possible and normalising audio volume.\n",
        "\n",
        "However, the dataset will still contain substantial real-world variations due to the nature of the recordings, they are affected by factors such as different microphones, different acoustic environments, just to name a few. \n",
        "\n",
        "Audio instances are high dimensional, and even 5 second clip samples at 16kHz contain around 80,000 raw waveform values. For this reason, raw audio cannot be used directly with ML models and feature extraction is required, this will be outlined in the methodology section. \n",
        "\n",
        "## 2.4 What makes the problem interesting:\n",
        "- High variability in humming and whistling: people hum and whistle differently, there is varibaility in pitch stability, tempo and rhythm. Even the same person can reproduce the same melody inconsistently. \n",
        "- Similarity between some melodies: certain songs in the dataset share similar rhythmic patterns.\n",
        "- Recording and environemntal noise: audio was captured through the same software, but with different devices, microphones and under different acoustic conditions. Despite preprocessing done by module team, variation remains.\n",
        "- Complex, high-dimensional input: raw audio is highly dimensional and relationship between raw waveform and melody is higly non-linear, which cannot be learned by classical methods. Featuer extraction is required to represent the instances better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPTSuaB9L2jU"
      },
      "source": [
        "# 3 Methodology\n",
        "## 3.1 Overview of methodology\n",
        "The overall methodology will follow the standard supervised learning pipeline for audio classification. \n",
        "- Loading the dataset. \n",
        "- Preprocessing raw audio waveform to extract meaningful features.\n",
        "- Training a range of machine learning models.\n",
        "- Optimising hyperparameters using the validation data.\n",
        "- Evaluating final performance on the test set.\n",
        "- Comparing models to reflect on results across them. \n",
        "\n",
        "## 3.2 Audio preprocessing\n",
        "As mentioned in the problem formulation, raw waveform data is highly dimensional and not very useful for machine learning algorithms. Preprocessing of the data will include:\n",
        "- Loading audio files using librosa at a fixed sampling rate of 16khz. \n",
        "- Normalising amplitude to reduce loudness variation.\n",
        "- Converting waveforms into 2D time-frequency representations using MFCCs and Mel spectograms. \n",
        "- Trimming to make sure all the instances are 10 seconds long, and have a consistent number of frames. \n",
        "- Scalling features.\n",
        "\n",
        "MFCCs will be used for Logistic Regression, KNN and MLP. \n",
        "\n",
        "Mel-spectograms will be used for CNNs to preserve spatial patterns required for convolution. \n",
        "\n",
        "## 3.3 Data splitting\n",
        "To ensure correct evaluation of the models, the larger dataset will be divided into:\n",
        "- Training set, used to fit model\n",
        "- Validation set, used for hyperparameter turning and model selection.\n",
        "- Testing set, used at the end to estimate genralisation performance.\n",
        "\n",
        "A stratified split will be used to maintian balanced class proportion for all 8 songs. The full methodology and implementation of data splitting is outlined in Ssection 5 - Datasets. \n",
        "\n",
        "## 3.4 Models considered\n",
        "A range of models will be implemented to examine how perfromance changes with model complexity. \n",
        "### 3.4.1 Logistic Regression\n",
        "A simple linear classifier, used for baseline to commpare more advanced models against. Logistic regression is fast to train and interpretable. It directly uses flattened extracted features from data preprocessing. \n",
        "\n",
        "Not expected to perform well as humming and whistling data is non linear. \n",
        "### 3.4.2 k-Nearest Neighbours (KNN)\n",
        "Non-parametric and memory based model which is easy to implement and useful for examining how the data clusters after feature extraction using MFCC. This model will be sensitive to feature scaling and high-dimensionality, but is expected to have better performance depending on k. \n",
        "### 3.4.3 Multi-layer Perceptron (MLP)\n",
        "Fully connected neural network that can model non linear relationships in the data. Also works from flattened features extracted using MFCC, but has risks of overfitting if hyperparameters are not tuned carefully. \n",
        "### 3.4.4 Convolutional Neural Network (CNN)\n",
        "Deep learning model capable of extracting spatial patterns from 2D representations. Highest expected performance due to ability to learn hierachical features. Not as easy to implement, due to factors such as regularisation, and more computationally expensive with added risk of overffiting on small datasets. \n",
        "\n",
        "## 3.5 Performance Metrics\n",
        "Model performance will be quanitified using:\n",
        "- Accuracy to determine the percentage of correctly classified test samples.\n",
        "- Confusion matrix to provide insights into which classes are being misclassified. \n",
        "- Loss curves for MLP and CNN to help diagnose overfitting. \n",
        "\n",
        "Accuracy, although not always best to evaluate models, is appropriate in this case as this is a balanced multi-class classification task. \n",
        "\n",
        "## 3.6 Validation Task\n",
        "Validation task inloves the selection of the best parameters such as k in KNN and learning rates. Comparing performance across models with these tuned parameters on the validation data. It is also where I will check if the model is overffiting, by monitoring training and validation accuracy, and selecting the model with best performance to take to the testing set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F5_kI95LuZ2"
      },
      "source": [
        "# 4 Implemented ML prediction pipelines\n",
        "The main idea of the prediction pipelines in this project is to convert raw audio recordings from the dataset into inputs suitable for machine learning models, mapping these representations to one of the eight song labels. \n",
        "\n",
        "The pipeline is a sequence of processing stages, each applying some transformation to the data into a new intermediate representation, which finally leads to a class label. Since I am going to implement 4 models, I have two main pipelines:\n",
        "\n",
        "- Pipeline A: used by Logistic Regression, k-Nearest Neighbours (kNN) and Multi-Layer Perceptron (MLP)\n",
        "- Pipeline B: used for Convolutional Neural Network\n",
        "\n",
        "The need for two pipelines comes from the fact that both share the same loading procedure, but differ in feature extraction stages.\n",
        "\n",
        "## 4.1 Transformation stage\n",
        "During this stage raw waveform audio data is converted into meaningful features which can be used for machine learning, as original data is too high-dimensional and unsuitable for the task. All audio files are loaded at a fixed sampling rate and their amplitude is normalised, and finally features are extracted. \n",
        "### 4.1.1 Pipeline A\n",
        "**Input:** audio waveform instances from the dataset\n",
        "**Transformations:**\n",
        "- Audio is trimmed to a fixed 10 second duration.\n",
        "- Mel-Frequency Cepstral Coefficient (MFCCs) are computed.\n",
        "- MFCC matrix is flattened by averaging across time interval (10 seconds)\n",
        "- Features are scaled using StandardScaler\n",
        "**Output:** Fixed-length feature vector suitable for machine learning models 1-3. \n",
        "\n",
        "After the transformation is done, MFCC has compressed the signal into a meaningful space while reducing dimensionality, so data can be used in this task. \n",
        "MFCC works by converting the audio into short frames and then looking at which frequencies are resent. The frequency spectrum is then compressed using triangular Mel filters. Logarithmic values are taken and most important coefficients are kept using DCT.\n",
        "\n",
        "Since the result is a vector, this makes it ideal for logistic regression, kNN and MLPs which use vector inputs. \n",
        "### 4.1.2 Pipeline B\n",
        "**Input:** audio waveform instances from the dataset\n",
        "**Transformations:**\n",
        "- Audio is trimmed to a fixed 10 second duration.\n",
        "- Mel-spectogram is generated\n",
        "- Log scaling is applied to stabilise for variance\n",
        "- Resulting 2D time-frequency array is normalised\n",
        "**Output:** 2D array representing a spectogram suitable for a convolution network. \n",
        "\n",
        "CNN looks at spatial locality and patterns in the 2D data. Using this method we preserve meaningful time-frequency relationships which are then used by CNN to learns hierarchical audio features which cant be accessed by simpler models. \n",
        "\n",
        "## 4.2 Model stage\n",
        "Eac processed feature representation is passed into a classification model, three of the models use the same input format which are MFCC vectors, and CNN uses a 2D spectogram array.\n",
        "\n",
        "### 4.2.1 Logistic Regression\n",
        "**Input:** Flattened MFCC vector\n",
        "\n",
        "**Output:** Probability distirbution over 8 classes \n",
        "\n",
        "Logistic regression acts as a simple model which can be used as a baseline and benchmarking for other models. It is also used to see if features extracted in previous stage contain enough information to be modelled linearly. \n",
        "\n",
        "### 4.2.2 k-Nearest Neighbours (kNN)\n",
        "**Input:** Flattened MFCC vector\n",
        "\n",
        "**Output:** Predicted class label from majority vote among k closest samples. \n",
        "\n",
        "Reveals natural cluster structure in MFCC (feature) space, non parametric baseline. Makes no assumption about the underlying data line Logistic regression did. \n",
        "\n",
        "### 4.2.3 Multi-layer Perceptron (MLP)\n",
        "**Input:** Flattened MFCC vector\n",
        "\n",
        "**Architecture:** Dense layers with nonlinear activations, ending with a softmax layer\n",
        "\n",
        "**Output:** Class probabilities \n",
        "\n",
        "Can learn non linear relationships between MFCC extracted features without requiring convolution. Unlike Logistic Regression, can model more complex relationships as it can learn non linear transformations, potentially expected to perform better. \n",
        "\n",
        "### 4.2.4 Convolutional Neural Network (CNN)\n",
        "**Input:** 2D mel-spectogram array\n",
        "\n",
        "**Architecture:** Convolution, activation, pooling, followed by dense layers.\n",
        "\n",
        "**Output:** Class probabilities\n",
        "\n",
        "Can capture time-frequency patterns and learn hierarcial representations, best for audio classification out of the three previous models.\n",
        "\n",
        "## 4.3 Ensemble stage\n",
        "\n",
        "For my mini project I will not implement an ensemble method. \n",
        "\n",
        "Techniques such as bagging, boosting or stacking can improve predicion performance of models as it combines multiple models. I chose to train multiple deep learning and classical models instead, as outlined above. \n",
        "\n",
        "The aim of my project is to compare individual model families like linear models, instance-based models, neural networks and convolutional networks. Implementing an ensemble stage on top of this would make it harder to evaluate the model families independently. \n",
        "\n",
        "Therefore, the model stage includes four models and pipelines that are described above and the ensemble stage is intentionally left out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZQPxztuL9AW"
      },
      "source": [
        "# 5 Dataset\n",
        "## 5.1 Overview of dataset construction\n",
        "All datasets that I will create in this section are derived from the MLEnd Hums and Whistles II dataset. For this project, the 800-sample version is used exclusively. The reason for this is because 800-sample dataset fully contains all samples from the 400 one, and more on top. Using the bigger dataset provides a larger, more diverse set of recordings, which is importants for training machine learning models. \n",
        "\n",
        "The original dataset has 800 audio recordings, each is either a hum or a whistle of one of eight song classes. Each file is labelled under the same convention that encodes both the song and whether the recording is a hum or a whistle. \n",
        "\n",
        "The goal of this section is to build the training, validation and testing datasets which will be used in the training and evaluation of models in this project. \n",
        "\n",
        "All raw audio files will be loaded, labelled, preprocessed and then split into subsets for training, validation and testing. \n",
        "\n",
        "Original dataset is balanced across the eight classes (100 instances per class).\n",
        "\n",
        "## 5.2 Ensuring IID, balance, independence\n",
        "- To ensure fair evaluation across all models all the files will first be shuffled. \n",
        "- Strattified sampling will then be used so that each class maintains equal representation in training, validationa and testing sets. \n",
        "- The splits are all mutually exclusive, meaning no audio clip appears in more than one dataset. \n",
        "- Instances are treated as independent and identically treated as each one is a separate human performance, there is not dependence between them, and metadata indicates unique recording per file. \n",
        "\n",
        "The split of data between the three parts for each dataset are:\n",
        "- 70% training\n",
        "- 15% validation\n",
        "- 15% testing\n",
        "\n",
        "Logistic Regression, kNN and MLP will be trained on training set of Dataset A. NLP parameters will be fine tuned on validation set of Database A. And all three models' performances will be evaluated on Dataset A's testing data. \n",
        "\n",
        "CNN will be trained on Dataset B's training data, fine-tuned with Dataset B's validation data and performance evaluated on Dataset B's testing data. \n",
        "\n",
        "## 5.3 Dataset A (MFCC-based)\n",
        "This follows Transformation stage of Pipeline A and is designed for classical ML models: Logistic Regression, k-Nearest Neighbours and MLP.\n",
        "\n",
        "Each audio file:\n",
        "1. Is loaded as waveform at a fixed sampling rate of 16kHz.\n",
        "2. Trimmed to a consistent duration fo 10 seconds. \n",
        "3. Normalised for amplitude. \n",
        "4. Compute MFCC features. \n",
        "5. MFCC time series is collapsed by averaging across time to form fixed length feature vector. \n",
        "6. Features are scaled using StandardScaler. \n",
        "\n",
        "The following NumPy arrays are created and stored:\n",
        "\n",
        "- X_train_mfcc.npy, X_val_mfcc.npy, X_test_mfcc.npy (1-D feature vectors)\n",
        "- y_train.npy, y_val.npy, y_test.npy (corresponsing integer labels, shared with Dataset B)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of audio files: 800\n",
            "Unique classes found: [np.str_('Feeling'), np.str_('Friend'), np.str_('Happy'), np.str_('Married'), np.str_('Necessities'), np.str_('NewYork'), np.str_('RememberMe'), np.str_('TryEverything')]\n",
            "MFCC dataset shape: (800, 40)\n",
            "Train size: (560, 40)\n",
            "Val size: (120, 40)\n",
            "Test size: (120, 40)\n",
            "Saved MFCC datasets in data/processed/\n"
          ]
        }
      ],
      "source": [
        "# imports \n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DATA_DIR = \"../data/MLEndHWII_Sample_800\" # original dataset path\n",
        "SAMPLE_RATE = 16000 # 16 kHz\n",
        "DURATION = 10  # 10 seconds\n",
        "FIXED_LENGTH = SAMPLE_RATE * DURATION  \n",
        "\n",
        "# loading filenames and sorting\n",
        "files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".wav\")]\n",
        "files.sort()\n",
        "\n",
        "print(\"Number of audio files:\", len(files))\n",
        "\n",
        "# file name format exmaple: S1_hum_2_Necessities.wav\n",
        "# file name format :        {sample_num}_{hum/whistle}_{class_label}.wav\n",
        "# extracting class labels from file names\n",
        "\n",
        "def extract_label(filename):\n",
        "    components = filename.split(\"_\") # split by underscore  \n",
        "    end = components[-1].split(\".\") # split last part by dot {class_label}.wav\n",
        "    class_label = end[0]  # get class label\n",
        "    return class_label\n",
        "\n",
        "labels = np.array([extract_label(f) for f in files])\n",
        "\n",
        "# sorting and converting label classes to integers for classification\n",
        "unique = sorted(list(set(labels)))\n",
        "label_to_int = {label: i for i, label in enumerate(unique)}\n",
        "y = np.array([label_to_int[l] for l in labels])\n",
        "\n",
        "print(\"Unique classes found:\", unique) # checking all class names exist\n",
        "\n",
        "X_mfcc = [] # list to hold MFCC feature vectors\n",
        "\n",
        "# loading audio files, padding/trimming to fixed length, computing MFCCs\n",
        "for f in files:\n",
        "    path = os.path.join(DATA_DIR, f)\n",
        "\n",
        "    # load audio using librosa\n",
        "    audio, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
        "\n",
        "    # pad/trim to fixed length\n",
        "    if len(audio) > FIXED_LENGTH:\n",
        "        audio = audio[:FIXED_LENGTH]\n",
        "    else:\n",
        "        audio = np.pad(audio, (0, FIXED_LENGTH - len(audio)))\n",
        "\n",
        "    # compute MFCC features and collapse to 1D vector\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40) # 40 coefficients\n",
        "    mfcc_mean = mfcc.mean(axis=1)\n",
        "\n",
        "    X_mfcc.append(mfcc_mean)\n",
        "\n",
        "X_mfcc = np.array(X_mfcc) # the whole dataset as numpy array of feature vectors\n",
        "print(\"MFCC dataset shape:\", X_mfcc.shape) \n",
        "\n",
        "\n",
        "# stratified splitting to maintain class distribution \n",
        "# 70% train, 15% val, 15% test\n",
        "# first split into train 70% and temp 30%\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_mfcc, y, test_size=0.30, random_state=42, stratify=y)\n",
        "\n",
        "# then split temp into val 15% and test 15%\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(\"Train size:\", X_train.shape)\n",
        "print(\"Val size:\", X_val.shape)\n",
        "print(\"Test size:\", X_test.shape)\n",
        "\n",
        "\n",
        "# standard scaling on training data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# saving processed datasets\n",
        "os.makedirs(\"../data/processed\", exist_ok=True)\n",
        "\n",
        "np.save(\"../data/processed/X_train_mfcc.npy\", X_train_scaled)\n",
        "np.save(\"../data/processed/X_val_mfcc.npy\", X_val_scaled)\n",
        "np.save(\"../data/processed/X_test_mfcc.npy\", X_test_scaled)\n",
        "\n",
        "np.save(\"../data/processed/y_train.npy\", y_train)\n",
        "np.save(\"../data/processed/y_val.npy\", y_val)\n",
        "np.save(\"../data/processed/y_test.npy\", y_test)\n",
        "\n",
        "print(\"Saved MFCC datasets in data/processed/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4 Dataset B (Spectogram-based)\n",
        "This follows Transformation stage of Pipeline B and is designed for CNN use only, which requires 2D structured inputs. \n",
        "\n",
        "Each audio file:\n",
        "1. Is loaded as waveform at a fixed sampling rate of 16kHz. \n",
        "2. Normalised for amplitude. \n",
        "3. Generates Mel-spectogram using short-time Fourier transform followed by Mel filterbanks. \n",
        "4. Log-scaled to stabilise variance.\n",
        "5. Resized to consistent shapes (equivalent of trimming to 10 seconds for Pipeline A, for consistency)\n",
        "6. Normalised.\n",
        "\n",
        "The following NumPy arrays are created and stored:\n",
        "\n",
        "- X_train_spec.npy, X_val_spec.npy, X_test_spec.npy (2D feature matrix)\n",
        "- y_train.npy, y_val.npy, y_test.npy (corresponsing integer labels, shared with Dataset A)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of audio files: 800\n",
            "Unique classes found: [np.str_('Feeling'), np.str_('Friend'), np.str_('Happy'), np.str_('Married'), np.str_('Necessities'), np.str_('NewYork'), np.str_('RememberMe'), np.str_('TryEverything')]\n",
            "Spectrogram dataset shape: (800, 64, 313)\n",
            "Train: (560, 64, 313)\n",
            "Val:   (120, 64, 313)\n",
            "Test:  (120, 64, 313)\n",
            "Saved spectrogram datasets in data/processed/\n"
          ]
        }
      ],
      "source": [
        "# imports \n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DATA_DIR = \"../data/MLEndHWII_Sample_800\" # original dataset path\n",
        "SAMPLE_RATE = 16000 # 16 kHz\n",
        "DURATION = 10  # 10 seconds\n",
        "FIXED_LENGTH = SAMPLE_RATE * DURATION  \n",
        "\n",
        "# loading filenames and sorting\n",
        "files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".wav\")]\n",
        "files.sort()\n",
        "\n",
        "print(\"Number of audio files:\", len(files))\n",
        "\n",
        "# file name format exmaple: S1_hum_2_Necessities.wav\n",
        "# file name format :        {sample_num}_{hum/whistle}_{class_label}.wav\n",
        "# extracting class labels from file names\n",
        "\n",
        "def extract_label(filename):\n",
        "    components = filename.split(\"_\") # split by underscore  \n",
        "    end = components[-1].split(\".\") # split last part by dot {class_label}.wav\n",
        "    class_label = end[0]  # get class label\n",
        "    return class_label\n",
        "\n",
        "labels = np.array([extract_label(f) for f in files])\n",
        "\n",
        "# sorting and converting label classes to integers for classification\n",
        "unique = sorted(list(set(labels)))\n",
        "label_to_int = {label: i for i, label in enumerate(unique)}\n",
        "y = np.array([label_to_int[l] for l in labels])\n",
        "\n",
        "print(\"Unique classes found:\", unique) # checking all class names exist\n",
        "\n",
        "# spectogram dataset\n",
        "X_spec = []\n",
        "\n",
        "# spectrogram parameters\n",
        "N_MELS = 64          # number of Mel filters\n",
        "HOP_LENGTH = 512     # controls time resolution\n",
        "FMIN = 20\n",
        "FMAX = SAMPLE_RATE // 2\n",
        "\n",
        "for f in files:\n",
        "    path = os.path.join(DATA_DIR, f)\n",
        "\n",
        "    # load audio using librosa\n",
        "    audio, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
        "\n",
        "    # pad/trim to fixed length\n",
        "    if len(audio) > FIXED_LENGTH:\n",
        "        audio = audio[:FIXED_LENGTH]\n",
        "    else:\n",
        "        audio = np.pad(audio, (0, FIXED_LENGTH - len(audio)))\n",
        "\n",
        "    # compute mel-spectrogram\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=audio,\n",
        "        sr=SAMPLE_RATE,\n",
        "        n_mels=N_MELS,\n",
        "        fmin=FMIN,\n",
        "        fmax=FMAX,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "\n",
        "    # convert to log scale\n",
        "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "    # normalise spectrogram\n",
        "    mel_norm = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
        "\n",
        "    X_spec.append(mel_norm)\n",
        "\n",
        "X_spec = np.array(X_spec)\n",
        "\n",
        "print(\"Spectrogram dataset shape:\", X_spec.shape)  \n",
        "# expected shape: (800, 64, ~310) depending on hop size\n",
        "\n",
        "# stratified splitting to maintain class distribution \n",
        "# 70% train, 15% val, 15% test\n",
        "# first split into train 70% and temp 30%\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_spec, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# then split temp into val 15% and test 15%\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Val:  \", X_val.shape)\n",
        "print(\"Test: \", X_test.shape)\n",
        "\n",
        "\n",
        "# saving processed datasets\n",
        "os.makedirs(\"../data/processed\", exist_ok=True)\n",
        "\n",
        "np.save(\"../data/processed/X_train_spec.npy\", X_train)\n",
        "np.save(\"../data/processed/X_val_spec.npy\", X_val)\n",
        "np.save(\"../data/processed/X_test_spec.npy\", X_test)\n",
        "\n",
        "np.save(\"../data/processed/y_train.npy\", y_train)\n",
        "np.save(\"../data/processed/y_val.npy\", y_val)\n",
        "np.save(\"../data/processed/y_test.npy\", y_test)\n",
        "\n",
        "print(\"Saved spectrogram datasets in data/processed/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qf7GN1aeXJI"
      },
      "source": [
        "# 6 Experiments and results\n",
        "\n",
        "Carry out your experiments here. Analyse and explain your results. Unexplained results are worthless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSrJCR_cekPO"
      },
      "source": [
        "# 7 Conclusions\n",
        "\n",
        "Your conclusions, suggestions for improvements, etc should go here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhJO9mAWQn50"
      },
      "source": [
        "# 8 References\n",
        "\n",
        "Acknowledge others here (books, papers, repositories, libraries, tools)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
